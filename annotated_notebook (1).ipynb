{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd30621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np\n",
    "from pathlib import Path\n",
    "from torch import tensor\n",
    "from fastcore.test import test_close\n",
    "torch.manual_seed(42)\n",
    "\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "torch.set_printoptions(precision=2, linewidth=125, sci_mode=False)\n",
    "np.set_printoptions(precision=2, linewidth=125)\n",
    "\n",
    "path_data = Path('data')\n",
    "path_gz = path_data/'mnist.pkl.gz'\n",
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c57e01",
   "metadata": {},
   "source": [
    "### Cell 1: Importing Libraries and Loading Data\n",
    "This cell performs several key tasks:\n",
    "1. **Imports Libraries:** Libraries such as `pickle`, `gzip`, `torch` (PyTorch), `matplotlib`, and `numpy` are imported for data processing, machine learning, and plotting.\n",
    "2. **Setting Up Environment:** Sets the random seed for PyTorch for reproducibility and configures matplotlib and print options for better data visualization.\n",
    "3. **Loading Data:** Loads the MNIST dataset from a gzipped pickle file, which contains handwritten digit images, and splits it into training and validation sets.\n",
    "4. **Converting to Tensors:** Converts the data into PyTorch tensors, which are multi-dimensional arrays optimized for machine learning operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4dcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n,m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "n,m,c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495faea0",
   "metadata": {},
   "source": [
    "### Cell 2: Defining Dimensions\n",
    "This cell extracts dimensions of the dataset:\n",
    "1. **Shape of Training Data:** Retrieves the number of samples (`n`) and features (`m`) in the training data.\n",
    "2. **Number of Classes:** Calculates the number of output classes (`c`) based on the maximum label in `y_train`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c024ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num hidden\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16947635",
   "metadata": {},
   "source": [
    "### Cell 3: Setting the Number of Hidden Units\n",
    "This cell sets the number of neurons (`nh`) in the hidden layer of the neural network. These neurons are intermediate processing units that help the network learn complex patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7788eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(m,nh)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh,1)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204d4902",
   "metadata": {},
   "source": [
    "### Cell 4: Initializing Weights and Biases\n",
    "This cell initializes the weights and biases for two layers of a neural network:\n",
    "1. **Weights and Biases for Layers:** Initializes weights (`w1`, `w2`) and biases (`b1`, `b2`) for the layers. Weights are randomly initialized, while biases are set to zero.\n",
    "2. **Dimensions:** Sets the dimensions of weights and biases according to the network architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin(x, w, b): return x@w + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a087688",
   "metadata": {},
   "source": [
    "### Cell 5: Defining a Linear Function\n",
    "This cell defines a linear function `lin` used in the neural network. It performs a matrix multiplication of the input `x` with the weights `w` and adds the bias `b`. This represents a linear transformation in the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad48ec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = lin(x_valid, w1, b1)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e239bff",
   "metadata": {},
   "source": [
    "### Cell 6: Applying the Linear Function\n",
    "This cell applies the linear transformation to the validation data:\n",
    "1. **Linear Transformation:** Uses the `lin` function with the validation data `x_valid`, weights `w1`, and bias `b1`. This is the output of the first layer of the neural network.\n",
    "2. **Output Shape:** Checks the shape of the transformed data `t` to ensure the operation was successful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a75922c",
   "metadata": {},
   "source": [
    "### Cell 7: Defining the ReLU Activation Function\n",
    "This cell defines the ReLU (Rectified Linear Unit) activation function:\n",
    "1. **ReLU Function:** ReLU is defined as `f(x) = max(0, x)`. It introduces non-linearity to the network, allowing it to learn complex patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31f63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = relu(t)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ae7ce",
   "metadata": {},
   "source": [
    "### Cell 8: Applying ReLU to the Linear Transformation\n",
    "This cell applies the ReLU activation function to the linear transformation:\n",
    "1. **Applying ReLU:** The ReLU function is applied to the data `t`, which is the output of the first neural network layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0226cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb):\n",
    "    l1 = lin(xb, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    return lin(l2, w2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d549f535",
   "metadata": {},
   "source": [
    "### Cell 9: Defining the Neural Network Model\n",
    "This cell defines the entire neural network model:\n",
    "1. **Model Definition:** The `model` function takes input and applies two layers: a linear transformation followed by ReLU, and then another linear transformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(x_valid)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee8023a",
   "metadata": {},
   "source": [
    "### Cell 10: Testing the Model on Validation Data\n",
    "This cell tests the neural network model on the validation data:\n",
    "1. **Model Testing:** The validation data `x_valid` is passed through the model to obtain the output `res`.\n",
    "2. **Result Shape:** Checks the shape of `res` to understand the output structure of the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e328a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape,y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff1bb0",
   "metadata": {},
   "source": [
    "### Cell 11: Checking Shapes of Model Output and Labels\n",
    "This cell checks the dimensions of the model output `res` and the validation labels `y_valid` to ensure compatibility for further calculations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e825908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(res-y_valid).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9a373",
   "metadata": {},
   "source": [
    "### Cell 12: Shape of Difference between Output and Labels\n",
    "Calculates and checks the shape of the difference between the model output and validation labels, a step typically part of loss computation in models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29297870",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[:,0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7842438",
   "metadata": {},
   "source": [
    "### Cell 13: Reshaping Model Output\n",
    "Reshapes the model output `res` to match the dimensionality of the labels, which is crucial for comparison or loss calculations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.squeeze().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93625f",
   "metadata": {},
   "source": [
    "### Cell 14: Squeezing Model Output\n",
    "Applies the `squeeze` method to `res` to remove dimensions of size 1, simplifying operations like loss calculation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c374bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "(res[:,0]-y_valid).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed86453",
   "metadata": {},
   "source": [
    "### Cell 15: Shape of Difference after Adjusting Dimensions\n",
    "After adjusting the dimensions of `res`, calculates the difference with `y_valid` again to ensure accurate loss computation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a198c1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,y_valid = y_train.float(),y_valid.float()\n",
    "\n",
    "preds = model(x_train)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145e44ce",
   "metadata": {},
   "source": [
    "### Cell 16: Preprocessing Labels and Getting Predictions\n",
    "Converts labels to floating-point numbers and gets predictions from the model using training data. Also checks the shape of the predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19351c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(output, targ): return (output[:,0]-targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e01cc3e",
   "metadata": {},
   "source": [
    "### Cell 17: Defining Mean Squared Error Function\n",
    "Defines the Mean Squared Error (MSE) function, a common loss function used in regression tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b60f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a623792",
   "metadata": {},
   "source": [
    "### Cell 18: Calculating MSE for Predictions\n",
    "Computes the MSE of the model's predictions against the actual training labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols,diff\n",
    "x,y = symbols('x y')\n",
    "diff(x**2, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95efd61b",
   "metadata": {},
   "source": [
    "### Cell 19: Symbolic Differentiation with SymPy\n",
    "Demonstrates symbolic differentiation using SymPy, a symbolic mathematics library.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e3943",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(3*x**2+9, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a00dae2",
   "metadata": {},
   "source": [
    "### Cell 20: Another Example of Symbolic Differentiation\n",
    "Performs symbolic differentiation on a polynomial expression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c048630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    # grad of matmul with respect to input\n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f00e1",
   "metadata": {},
   "source": [
    "### Cell 21: Defining Gradient for Linear Layer\n",
    "Defines a function to compute gradients for a linear layer, an essential part of backpropagation in neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be821224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward pass:\n",
    "    l1 = lin(inp, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w2, b2)\n",
    "    diff = out[:,0]-targ\n",
    "    loss = diff.pow(2).mean()\n",
    "    \n",
    "    # backward pass:\n",
    "    out.g = 2.*diff[:,None] / inp.shape[0]\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    l1.g = (l1>0).float() * l2.g\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f53a9",
   "metadata": {},
   "source": [
    "### Cell 22: Forward and Backward Pass\n",
    "Implements the forward and backward passes of the neural network, including loss calculation and backpropagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c98d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8be86",
   "metadata": {},
   "source": [
    "### Cell 23: Running Forward and Backward Pass\n",
    "Executes the forward and backward pass functions using training data and labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for testing against later\n",
    "def get_grad(x): return x.g.clone()\n",
    "chks = w1,w2,b1,b2,x_train\n",
    "grads = w1g,w2g,b1g,b2g,ig = tuple(map(get_grad, chks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf9242",
   "metadata": {},
   "source": [
    "### Cell 24: Storing Gradients for Testing\n",
    "Saves the gradients of weights, biases, and input for later comparison or testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5ade6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkgrad(x): return x.clone().requires_grad_(True)\n",
    "ptgrads = w12,w22,b12,b22,xt2 = tuple(map(mkgrad, chks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2bc485",
   "metadata": {},
   "source": [
    "### Cell 25: Preparing Variables for PyTorch Gradient Calculation\n",
    "Creates copies of variables with gradient tracking enabled for use with PyTorch's automatic differentiation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ad725",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,y_valid = y_train.float(),y_valid.float()\n",
    "\n",
    "preds = model(x_train)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ea4f3",
   "metadata": {},
   "source": [
    "### Cell 16: Preprocessing Labels and Getting Predictions\n",
    "Converts labels to floating-point numbers for loss calculations and gets model predictions from the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f28e219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(output, targ): return (output[:,0]-targ).pow(2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3611c53c",
   "metadata": {},
   "source": [
    "### Cell 17: Defining Mean Squared Error Function\n",
    "Defines the Mean Squared Error (MSE) function, a common loss function in regression tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae528266",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(preds, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f3a7a",
   "metadata": {},
   "source": [
    "### Cell 18: Calculating MSE for Predictions\n",
    "Computes the MSE for the model's predictions against the actual training labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1bd27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols,diff\n",
    "x,y = symbols('x y')\n",
    "diff(x**2, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad5e32",
   "metadata": {},
   "source": [
    "### Cell 19: Symbolic Differentiation with SymPy\n",
    "Performs symbolic differentiation of `x**2` with respect to `x` using SymPy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b03c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(3*x**2+9, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a933c68",
   "metadata": {},
   "source": [
    "### Cell 20: Another Example of Symbolic Differentiation\n",
    "Calculates the derivative of the polynomial `3*x**2 + 9` with respect to `x`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    # grad of matmul with respect to input\n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b789be9",
   "metadata": {},
   "source": [
    "### Cell 21: Defining Gradient for Linear Layer\n",
    "Defines a function to compute gradients for a linear layer during backpropagation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764f4ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward pass:\n",
    "    l1 = lin(inp, w1, b1)\n",
    "    l2 = relu(l1)\n",
    "    out = lin(l2, w2, b2)\n",
    "    diff = out[:,0]-targ\n",
    "    loss = diff.pow(2).mean()\n",
    "    \n",
    "    # backward pass:\n",
    "    out.g = 2.*diff[:,None] / inp.shape[0]\n",
    "    lin_grad(l2, out, w2, b2)\n",
    "    l1.g = (l1>0).float() * l2.g\n",
    "    lin_grad(inp, l1, w1, b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a0ca9a",
   "metadata": {},
   "source": [
    "### Cell 22: Forward and Backward Pass\n",
    "Implements the forward and backward passes of the neural network for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8294704",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02867ebc",
   "metadata": {},
   "source": [
    "### Cell 23: Running Forward and Backward Pass\n",
    "Executes the forward and backward passes with training data and labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dac08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for testing against later\n",
    "def get_grad(x): return x.g.clone()\n",
    "chks = w1,w2,b1,b2,x_train\n",
    "grads = w1g,w2g,b1g,b2g,ig = tuple(map(get_grad, chks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32053237",
   "metadata": {},
   "source": [
    "### Cell 24: Storing Gradients for Testing\n",
    "Stores the gradients of weights, biases, and input for later comparison or testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f3ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkgrad(x): return x.clone().requires_grad_(True)\n",
    "ptgrads = w12,w22,b12,b22,xt2 = tuple(map(mkgrad, chks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28111f57",
   "metadata": {},
   "source": [
    "### Cell 25: Preparing Variables for PyTorch Gradient Calculation\n",
    "Prepares tensors for PyTorch's automatic differentiation by enabling gradient tracking.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
